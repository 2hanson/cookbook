---
title:      The Random Attribute
created_at: 2010-05-12 10:05:24.036546 -04:00
recipe: true
author: Alberto Lerner and Dwight Merriman
description:
filter:
 - erb
 - markdown
---

### Problem

Consider a scenario where you'd like to issue a query but would like
to pick a random document in the result.

<% code 'javascript' do %>
photos.find({"author":"johndoe"})
<% end %>

Any 'johndoe' would do. But you'd like a different one each time you
run the query. Sure, you can always count the resulting documents and
randomly pick one. But in that case the query would be run in its
entirety and all the results would be transferred to your app.

Now, consider another scenario where you'd like to run a map/reduce
but would be happy to trade result accuracy for performance. That is,
you'd be happy to use a sample (of a given percentage) of your data.
You don't really know the number of documents involved, but they are
numerous.


### Solution

An extra attribute in each document can help here. Simply make sure
that every time you insert a new document, you also give it a random
number (for instance, generated by Math.random() in
JavaScript). Unlike the _id, that number doesn't need to be unique. It
just needs to be, well, random.


### 1. Picking a random document from the result

If you're just interest in one document, you can generate a random
number on the fly and test it against the stored attribute.

<% code 'javascript' do %>
> db.docs.drop()
> db.docs.save( { key : 1, random : Math.random() } )
> db.docs.save( { key : 2, random : Math.random() } )
> db.docs.save( { key : 2, random : Math.random() } )
> db.docs.save( { key : 2, random : Math.random() } )

> rand = Math.random()
> db.docs.find( { key : 2, random : { $lte : rand } } ).limit(1)
> db.docs.find( { key : 2, random : { $gte : rand } } ).limit(1)
<% end %>

Note that sometimes a result may exist even though '$lte' returned
empty. In that case, you want to try '$gte' with the same 'rand'
before assuming the result is empty.

If this query is very important, you may speed it up with an index.

<% code 'javascript' do %>
> db.docs.ensureIndex( { x : 1, random :1 } )
<% end %>


### 2. Map/reduce on a sample of the data

If your collection is large and the computation you want to run could
operate on a sample instead, you can have the mapping phase apply an
early filter based on our 'random' attribute.

<% code 'javascript' do %>
> db.docs.drop()
> for (i=0; i < 10000; i++) { db.docs.save( { x : 1, y : Math.random() } ) }
> m = function() { emit(this.random, 1); }
> r = function(k, vals) {
  var sum=0;
  for (var i in vals) sum += vals[i];
  return sum;
 }

> sample = 0.05
> res = db.docs.mapReduce(m, r, { query : { y : { $lte: sample } } })
<% end %>

This will still issue the query over all the data but the mapper would
be called only for the sampled documents -- 5% of them here.

A nice property about this technique are that if you increase the
sample size, that would include the docs you picked on the smaller
sample, so you can tighten your confidence degree
incrementally. Another desirable property is that if your collection
changes in size, so would the sample.
